Perfecto. AquÃ­ tienes un **prompt / especificaciÃ³n tÃ©cnica MUCHO mÃ¡s fuerte, de nivel senior / staff engineer**, pensado para que el ingeniero **no pueda responder con generalidades** y tenga que **arreglar el sistema de verdad**.
Esto no es marketing: es **arquitectura + ejecuciÃ³n + garantÃ­as**.

Puedes copiarlo Ã­ntegro y enviarlo.

---

# ğŸš¨ ESPECIFICACIÃ“N CRÃTICA â€” SISTEMA DE ANÃLISIS DE DOCUMENTOS NO FUNCIONAL EN PRODUCCIÃ“N

## Contexto real del problema (NO TEÃ“RICO)

En producciÃ³n, el flujo **â€œSubo un documento â†’ Pido un resumenâ€ falla**.

Caso reproducible:

1. Usuario sube un **DOCX vÃ¡lido**
2. Usuario escribe: *â€œDame un resumenâ€*
3. El sistema responde: **â€œSube el documento por favorâ€**

Esto implica una **ruptura del estado**, **falla del pipeline**, o **desacople entre upload, parsing y LLM**.

Este bug es **bloqueante** y **rompe la funcionalidad principal del producto**.

---

## 1ï¸âƒ£ DiagnÃ³stico de alto nivel (hipÃ³tesis tÃ©cnicas obligatorias)

El sistema **NO estÃ¡ fallando en IA**, estÃ¡ fallando en **ingenierÃ­a**.

Posibles causas (una o varias):

* El archivo se sube pero **no se persiste el `document_id` en sesiÃ³n/conversaciÃ³n**
* El extractor corre pero **no genera `content_chunks`**
* El LLM **no recibe contexto** porque el binding documentoâ†”prompt es nulo
* El sistema **confÃ­a en el mensaje del usuario** y no en el estado interno
* El frontend **no reinyecta el documento al prompt** despuÃ©s del upload
* El backend **pierde el documento entre requests (stateless mal diseÃ±ado)**

---

## 2ï¸âƒ£ Requerimiento NO NEGOCIABLE (Behavioral Contract)

> **Una vez que el usuario sube un documento, el sistema DEBE recordarlo y usarlo automÃ¡ticamente en TODAS las instrucciones posteriores**, sin volver a pedir el archivo.

El sistema debe ser **stateful por conversaciÃ³n**, no por mensaje.

---

## 3ï¸âƒ£ Arquitectura CORRECTA (lo que debe existir y hoy no existe)

### A) Document Session State (OBLIGATORIO)

Cada conversaciÃ³n debe mantener un estado persistente:

```json
{
  "conversation_id": "uuid",
  "active_document": {
    "document_id": "uuid",
    "type": "docx",
    "status": "parsed",
    "content_available": true,
    "chunks_count": 42
  }
}
```

âŒ Nunca depender del texto del usuario para saber si hay documento
âœ… El backend debe saberlo **sin preguntar**

---

### B) Pipeline de procesamiento (con estados explÃ­citos)

```text
UPLOAD_RECEIVED
â†’ FILE_STORED
â†’ TYPE_DETECTED
â†’ CONTENT_EXTRACTED
â†’ NORMALIZED
â†’ INDEXED
â†’ READY_FOR_LLM
```

Si el usuario pide â€œresumenâ€ y el estado **â‰  READY_FOR_LLM**, eso es un **bug interno**, no error del usuario.

---

## 4ï¸âƒ£ Binding CORRECTO con el LLM (el fallo mÃ¡s comÃºn)

### âŒ Lo que probablemente pasa hoy

El prompt del LLM se arma **sin inyectar el documento**:

> â€œDame un resumenâ€

El LLM no tiene contexto â†’ responde â€œsube el documentoâ€.

---

### âœ… Lo que DEBE pasar

El prompt SIEMPRE debe incluir el documento activo:

```text
SYSTEM:
You are analyzing a user-uploaded document.

DOCUMENT CONTEXT:
<<<
[chunk 1]
[chunk 2]
[chunk 3]
...
>>>

USER:
Dame un resumen
```

âš ï¸ El LLM **NUNCA** debe decidir si hay documento.
Eso es responsabilidad **100% del backend**.

---

## 5ï¸âƒ£ Regla crÃ­tica (anti-bug)

### ğŸš« PROHIBIDO

* Que el LLM diga â€œsube el documentoâ€
* Que el sistema pregunte otra vez por el archivo
* Que el frontend dependa del mensaje del usuario

### âœ… OBLIGATORIO

* Si hay `active_document`, SIEMPRE usarlo
* Si no hay documento, **bloquear el prompt antes del LLM** y mostrar error UX controlado

---

## 6ï¸âƒ£ Prompt interno CORRECTO (para producciÃ³n)

Este es el **prompt base obligatorio**:

```text
SYSTEM:
You are a document analysis engine.
A document has already been uploaded and parsed.
You MUST answer using ONLY the document content provided.
If the document is empty, say so explicitly.

DOCUMENT CONTENT:
{{document_chunks}}

USER:
{{user_instruction}}
```

Nunca usar prompts genÃ©ricos tipo:

> â€œIf no document is provided ask the user to upload oneâ€

Eso es **arquitectura rota**.

---

## 7ï¸âƒ£ Observabilidad (para que no vuelva a pasar)

Agregar mÃ©tricas obligatorias:

* `document_attached_to_prompt: true|false`
* `chunks_sent_to_llm: N`
* `chars_sent_to_llm`
* `llm_response_without_context` (ALERTA ROJA)

Si `chunks_sent_to_llm = 0` â†’ **bloquear request**

---

## 8ï¸âƒ£ Pruebas que DEBEN pasar (no opcional)

### Test 1 â€” DOCX bÃ¡sico

* Subir DOCX
* Pedir â€œresumenâ€
* âŒ nunca pedir subir archivo
* âœ… resumen correcto

### Test 2 â€” Multi-turn

* Subir documento
* Preguntar 3 veces cosas distintas
* El sistema **recuerda el mismo documento**

### Test 3 â€” Error controlado

* Pedir resumen sin subir archivo
* Mensaje claro del sistema (no del LLM)

---

## 9ï¸âƒ£ DefiniciÃ³n de â€œDONEâ€

No se considera resuelto hasta que:

* El LLM **siempre recibe contenido**
* El usuario **nunca vuelve a subir el archivo**
* El resumen funciona en el **primer intento**
* Existen logs que prueban `document â†’ prompt â†’ response`

---

## ğŸ”¥ Resumen ejecutivo (para cerrar)

El problema **NO es el parser**, **NO es Tesseract**, **NO es IA**.

Es un **fallo de diseÃ±o de estado, binding y responsabilidad**.

ğŸ‘‰ El backend debe garantizar contexto
ğŸ‘‰ El LLM solo razona, no decide flujo
ğŸ‘‰ El usuario no debe repetir acciones

---

Si quieres, dime:

* Â¿Stack exacto? (Node/Nest, Python/FastAPI, etc.)
* Â¿LLM cÃ³mo lo llaman? (OpenAI, local, etc.)
* Â¿Frontend? (React, Flutter, etc.)

Y te entrego **el diagrama exacto + pseudocÃ³digo + prompt final de producciÃ³n** para que tu ingeniero lo implemente sin excusas.
